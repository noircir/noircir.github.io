---
title: "Using CGP Cluster to run Jupyter Notebook, RStudio, and Cloudera Quickstart"
author: ""
date: "2019-09-15"
output: html_document
---

<link rel="stylesheet" href="styles.css" type="text/css">

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Using CGP Cluster to run Jupyter Notebook, RStudio, and Cloudera Quickstart

Clusters are usually used for compute-intensive deep learning algorithms. When creating a cluster, CGP automatically installs Hadoop, Spark, Hive, and Pig (their version updates TBD). 

However, the same cluster (since it is a virtual machine) can run other applications and docker images. It is economical (proof).


I am going to istall Jupyter Notebook, RStudio, and Cloudera CDH Quickstart docker image. This image has the same Hadoop, Spark, Hive, and Pig, but also, in addition, has HBase and is optimized for Hue interface.

### Part 1. Creating a cluster on GCP and Installing Jupyter Notebook. 

1. Create a project , then Dataproc -> Create Cluster / changed only zone to northeast-.... and change storage type to SSD

2. Choose correct cluster version: the one with Python3 is  __[Cluster version 1.4](https://cloud.google.com/dataproc/docs/tutorials/python-configuration#image_version_14)__

3. Choose whether you need Anaconda or Miniconda: 
  (__[difference betwen Anaconda and Miniconda](http://deeplearning.lipingyang.org/2018/12/23/anaconda-vs-miniconda-vs-virtualenv/)__)

4 Pre-create a bucket with a name. designate the bucket. The jupyter notebooks will be saved in /bucket/notebooks/jupyter/

Create. 

5. Go to web interface, click on Jupyter, and voila. 

it is also a normal virtual machine, where you can access with SSH:

```
gcloud compute ssh \
    --zone=northamerica-northeast1-c \
    --project=drycluster \
    cluster-7676-m
```

you will be local user but on that machine

Check hadoop version
spark-shell (image) (-> scala)
hive 
no hbase (!)
pig version 0.17.0



### Install RStudio server

__[](https://cloud.google.com/solutions/running-rstudio-server-on-a-cloud-dataproc-cluster?authuser=1)__

image of Debian 9 commands
```
$ sudo apt-get install gdebi-core
$ wget https://download2.rstudio.org/server/debian9/x86_64/rstudio-server-1.2.1335-amd64.deb
$ sudo gdebi rstudio-server-1.2.1335-amd64.deb
```

Add user abby/abby123  (for RStudio)

issue a gcloud command from your local computer 

```
gcloud compute ssh \
    --zone=northamerica-northeast1-c \
    --project="drycluster" \
    "cluster-7676-m" -- \
    -L 8787:localhost:8787
```

to view all these project values, GCP provides hints, just in different places :)

In case of Dataproc Cluster where we are, the hints are in Cluster details -> VM instances -> SSH -> "view gcloud command"


The R files are saved in RStudio user (that we created) home directory: /home/abby/

- You can install packages in RStudio the usual way
For example, knitr: Install -> CRAN packages window appears, start installation.


### Installing Cloudera CDH Quickstart 

Follow the same instructions as 

__[Cloudera Quickstart on Google Cloud Platform - Part 1](2019-09-05-ClouderaOnGCP.html)__

__[Cloudera Quickstart on Google Cloud Platform - Part 2: Getting Hue to Work](2019-09-05-ClouderaOnGCP-2.html)__